---
title: "LDA Topics Models in Posts from Antiwork and TwoXChromosomes"
format: 
  gfm: 
    toc: true
---

```{r}
#| include: false
library(tidyverse)
library(arrow)
library(knitr)
library(pals)
AW <- read_parquet("data/AWl.parquet")
AWl_LDAs <- readRDS("data/AWl_LDAs.rds")
XX <- read_parquet("data/XXl.parquet")
XXl_LDAs <- readRDS("data/XXl_LDAs.rds")
XXAW <- read_parquet("data/XXAW.parquet")
XXAW15_LDAs <- readRDS("data/XXAW15_LDAs.rds")
```


## Data: All posts, selected posts (> 350 words), 3 corpora (Antiwork, TwoXChromosomes, Antiwork+TwoXChromosomes)

All posts from the subreddits [Antiwork](https://www.reddit.com/r/antiwork/) and [TwoXChromosomes](https://www.reddit.com/r/TwoXChromosomes/) have been collected using the data dumps of pushshift in 2023. This includes post from the very beginning of the subreddits until the end of 2022. 

For both data sets we then selected relevant variables, in particular the `title` string and the `selftext` string which is the post's text. We joined both stings into one sting and counted the words. Then we selected only those posts where title and text together have more than 350 words (additionally posts should not be marked as removed from reddit in the dataset). The strings in these texts where then used to construct a corpus for the topic models. (That means we tokenized the text and removed stop words with a slighlty customized selection of words as documented in the script `tomotopy_create_corpus.py`.)

For **Antiwork** there were 292.352 posts in the raw data. 132.617 of these have a non-empty seleftext. Combining title and selftext **22.629** posts have more than 350 words and build the documents in the Antiwork corpus. The number of words of the texts in the documents of the corpus range from 351 to 7875 (with mean 629.4 and median 520 words).

For **TwoXChromosomes** there were 420.486 posts in the raw data. 239.744 of these have a non-empty seleftext. Combining title and selftext **61.601** posts have more than 350 words and build the documents in the TwoXChromosomes corpus. The number of words of the texts in the documents of the corpus range from 351 to 7263 (with mean 613.4 and median 501 words).

Number of posts in Antiwork and TwoXChromosomes:

```{r}
#| echo: false
#| fig.height: 6
read_csv("data/XXAW_count_posts.csv", show_col_types = FALSE) |>
 ggplot(aes(x = `Time (monthly)`, y = Count, color = Type)) + geom_line() + facet_wrap(~Subreddit, scales = "free_y", nrow = 2) + 
 ggtitle("Number of posts") + theme_minimal() 
```

The number of posts in Antiwork before 2018 is quite low. Only occasionally one per month ended up in our corpus:
```{r}
#| echo: false
#| fig.height: 2.5
read_csv("data/XXAW_count_posts.csv", show_col_types = FALSE) |>
 filter(`Time (monthly)` < "2018-01-01", Subreddit == "Antiwork") |>
 ggplot(aes(x = `Time (monthly)`, y = Count, color = Type)) + geom_line() + 
 ggtitle("Antiwork") + theme_minimal() 
```


Finally we combined the two corpora into one corpus **Antiwork+TwoXChromosomes** with **84.230** documents. 

The Antiwork corpus has a vocabulary of `r read_csv("data/AWl1/vocab.csv", show_col_types = FALSE) |> nrow() |> format(big.mark = ".")` words, the TwoXChromosomes corpus has a vocabulary of `r read_csv("data/XXl1/vocab.csv", show_col_types = FALSE) |> nrow()|> format(big.mark = ".")` words and the combined corpus has a vocabulary of `r read_csv("data/XXAW151/vocab.csv", show_col_types = FALSE) |> nrow()|> format(big.mark = ".")` words.


## 10 LDA topic models for each corpus

We have created 10 topic models for each corpus (Antiwork, TwoXChromosomes and Antiwork+TwoXChromosomes) using the LDA model estimated with `tomotopy`. 
For the Antiwork and the TwoXChromosomes corpus we estimated LDAs with 10 topics. For the combined corpus we estimated LDAs with 15 topics. 

The following table shows the links to the visualizations of the topics of all LDAs constructed with `pyLDAvis`. 
The visualizations are interactive and allow to explore the topics and the words in the topics. 
There is a parameter $\lambda$ which can be adjusted to show the words of the topics arranged 
by the most common words in the topic ($\lambda = 1$), by the most common words as a fraction of their appearance in the whole corpus ($\lambda = 0$) or a mix between the two. 
The links below show the topic's words with $\lambda = 0.5$.

Antiwork LDAs | TwoXChromosomes LDAs | Antiwork+TwoXChromosomes LDAs |
--------------|----------------------|-------------------------------|
[Antiwork 1](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl1.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 1](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl1.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 1](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW151.html#topic=0&lambda=0.5&term=)   |
[Antiwork 2](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl2.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 2](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl2.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 2](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW152.html#topic=0&lambda=0.5&term=)   |
[Antiwork 3](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl3.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 3](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl3.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 3](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW153.html#topic=0&lambda=0.5&term=)   |
[Antiwork 4](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl4.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 4](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl4.html#topic=0&lambda=0.5&term=)    | [**Antiwork+TwoXChromosomes 4**](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW154.html#topic=0&lambda=0.5&term=)   |
[Antiwork 5](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl5.html#topic=0&lambda=0.5&term=)   | [**TwoXChromosomes 5**](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl5.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 5](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW155.html#topic=0&lambda=0.5&term=)   |   
[Antiwork 6](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl6.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 6](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl6.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 6](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW156.html#topic=0&lambda=0.5&term=)   |     
[Antiwork 7](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl7.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 7](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl7.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 7](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW157.html#topic=0&lambda=0.5&term=)   |     
[Antiwork 8](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl8.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 8](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl8.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 8](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW158.html#topic=0&lambda=0.5&term=)   |    
[Antiwork 9](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl9.html#topic=0&lambda=0.5&term=)   | [TwoXChromosomes 9](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl9.html#topic=0&lambda=0.5&term=)    | [Antiwork+TwoXChromosomes 9](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW159.html#topic=0&lambda=0.5&term=)   |  
[**Antiwork 10**](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl10.html#topic=0&lambda=0.5&term=) | [TwoXChromosomes 10](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl10.html#topic=0&lambda=0.5&term=)  | [Antiwork+TwoXChromosomes 10](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW1510.html#topic=0&lambda=0.5&term=) |

For each LDA perplexity scores and log-likelihood per word are calculated. The numbers are shown in a table in the Appendix below. 
The lower the perplexity and the higher the log-likelihood per word, the better the model. 
Based on these numbers we can select the best fitting LDA for each corpus. 
Consistently between the two measures, the best fitting LDA for Antiwork is 10, for TwoXChromosomes it is 5, and for Antiwork+TwoXChromosomes 4. 
Those are highlighted in the table above and used as the main reference in the following. 

The others LDAs serve to assess the robustness of the results because LDAs tend to not be converging to exactly the same solution when run again. 

## Subreddit Identity Topics

For both subreddits one topic can be interpreted as the subreddit identity topic. 

### Antiwork

[Antiwork LDA 10, Topic 3](https://janlorenz.github.io/TopicModels_XX_Antiwork/AWl10.html#topic=3&lambda=0.5&term=)  
is about "we" "our" "us" "workers" "society" as top words and also has the words "strike" "capitalism" "movement" and others in it. 
It is also the topic with the word "we" scoring highest and by the context this seems to relate to a collective "we" of the subreddit or a community transcending it. 

The Antiwork identity topic appears very similar as [Antiwork+TwoXChromosomes 4, Topic 4](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW154.html#topic=4&lambda=0.5&term=)

### TwoXChromosomes

[TwoXChromosomes LDA 5, Topic 6](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXl5.html#topic=6&lambda=0.5&term=)  
is about "women" "men" "woman" "people" "male" as top words and also has the words "feminism" "sexism" "trans" and others in it. 
The word "we" is not in the top words (although it is also not marginal in the topic). 
This is because other topics have the word "we" in it but these are the topics about family and relationships where the context of the "we" 
indicates that "we" relates to family or the relationship and not to a group identity of the subreddit. 

The TwoXChromosomes identity topic appears very similar in [Antiwork+TwoXChromosomes 4, Topic 12](https://janlorenz.github.io/TopicModels_XX_Antiwork/XXAW154.html#topic=12&lambda=0.5&term=)



# FROM HERE WORK IN PROGESS

## Robustness of LDAs

So, many of the topics in LDA 4 have a one-to-one correspondence with topics in many other LDAs. Typically 13 out of 15 topics from LDA4 appear in each other LDA (not always the same) as a clear one-to-one correspondance.
Over all other 9 LDA all topics in the majority of these 9 LDAs as a one-to-one correspondance with the topic in LDA4, often in 8 or 9 out of nine.  
 
...

## Antiwork LDA 10

```{r}
doc_topic <- AWl_LDAs$doc_topic_dists[[10]]
AW_top <- AW |> bind_cols(doc_topic)
AW_top |> mutate(year = year(created_time)) |> 
 group_by(year) |> summarise(across(paste0("T", 1:10), sum)) |> 
 pivot_longer(cols = paste0("T", 1:10), names_to = "topic", values_to = "count") |>
 mutate(topic = factor(topic, levels = paste0("T", 1:10))) |> 
 ggplot(aes(x = year, y = count, fill=topic)) + 
 geom_col(position = "stack") +
 scale_fill_manual(values = pals::glasbey(10)) +
 labs(title = "Presence of topics over all documents by year")
```



## TwoXChromosomes LDA 10


```{r}
doc_topic <- XXl_LDAs$doc_topic_dists[[5]]
XX_top <- XX |> bind_cols(doc_topic)
XX_top |> mutate(year = year(created_time)) |> 
 group_by(year) |> summarise(across(paste0("T", 1:10), sum)) |> 
 pivot_longer(cols = paste0("T", 1:10), names_to = "topic", values_to = "count") |>
 mutate(topic = factor(topic, levels = paste0("T", 1:10))) |> 
 ggplot(aes(x = year, y = count, fill=topic)) + 
 geom_col(position = "stack") +
 scale_fill_manual(values = pals::glasbey(10)) +
 labs(title = "Presence of topics over all documents by year")
```




## Antiwork+TwoXChromosomes LDA 4: Topics and documents

Most topics are typical for one of the subreddits. (All this is for LDA 4.)

```{r}
doc_topic <- XXAW15_LDAs$doc_topic_dists[[4]]
XXAW_top <- XXAW |> bind_cols(doc_topic)
XXAW_top |> group_by(subreddit) |> summarise(across(paste0("T", 1:15), sum)) |> 
 pivot_longer(cols = -subreddit, names_to = "topic", values_to = "count") |> 
 ggplot(aes(y=factor(topic, levels = paste0("T", 1:15)) |> fct_rev(), x=count, fill=subreddit)) + 
 geom_col(position = "stack") +
 facet_wrap(~subreddit) + 
 labs(y = "", title = "Presence of topics over all documents by subreddit", 
      caption = "Topic probabilities sum up to 1 for each post.\nThe barcharts can be interpreted as counting posts\nwhere each post contributes the fraction the topic has in it.") + 
 guides(fill = "none")
```


```{r}
doc_topic <- XXAW15_LDAs$doc_topic_dists[[4]]
XXAW_top <- XXAW |> bind_cols(doc_topic)
XXAW_top |> group_by(subreddit) |> summarise(across(paste0("T", 1:15), sum)) |> 
 pivot_longer(cols = -subreddit, names_to = "topic", values_to = "count") |> 
 mutate(Fraction = count/sum(count), .by = subreddit) |> 
 ggplot(aes(y=factor(topic, levels = paste0("T", 1:15)) |> fct_rev(), x=Fraction, fill=subreddit)) + 
 geom_col(position = 'dodge') +
 labs(y = "", title = "Presence of topics over all documents by subreddit as percentage in subreddit") + 
 guides(fill = "none")
```
There are only two topics which appear more or less equally often in both subreddits: Topic 13, about school time (roughly 3%) and topic 11 about police (roughly 2.5%). The clearly antiwork dominated topics are 2, 4, 5 and 7. The other topics are XX dominated: 1, 3, 6, 8, 9, 10, 12, 14, and 15.   

```{r}
doc_topic <- XXAW15_LDAs$doc_topic_dists[[4]]
XXAW_top <- XXAW |> bind_cols(doc_topic)
XXAW_top |> group_by(subreddit) |> summarise(across(paste0("T", 1:15), sum)) |> 
 pivot_longer(cols = -subreddit, names_to = "topic", values_to = "count") |> 
 mutate(Fraction = count/sum(count), .by = subreddit) |> 
 mutate(Fraction_topic = Fraction/sum(Fraction),
        Fraction_XX = Fraction_topic[1], 
        .by = topic) |> 
 ggplot(aes(y=factor(topic, levels = paste0("T", 1:15)) |> fct_rev() |> fct_reorder(Fraction_XX), x=Fraction_topic, fill=subreddit)) + 
 geom_col(position = 'stack') +
 labs(y = "", title = "Topic frequency within subreddits normalized per topic\nordered from most AW to most XX dominated") + 
 guides(fill = "none")
```



```{r}
#XXAW_top |> arrange(desc(num_comments)) |> select(subreddit, everything()) |> head(20)
```



```{r}
XXAW_top |> mutate(year = year(created_time)) |> 
 group_by(year) |> summarise(across(paste0("T", 1:15), sum)) |> 
 pivot_longer(cols = paste0("T", 1:15), names_to = "topic", values_to = "count") |>
 mutate(topic = factor(topic, levels = paste0("T", 1:15))) |> 
 ggplot(aes(x = year, y = count, fill=topic)) + 
 geom_col(position = "stack") +
 scale_fill_manual(values = pals::glasbey(15)) +
 labs(title = "Presence of topics over all documents by year")
```

```{r}
XXAW_top |> mutate(year = year(created_time)) |> 
 group_by(year) |> summarise(across(paste0("T", 1:15), mean)) |> 
 pivot_longer(cols = paste0("T", 1:15), names_to = "topic", values_to = "count") |>
 mutate(topic = factor(topic, levels = paste0("T", 1:15))) |> 
 ggplot(aes(x = year, y = count, fill=topic)) + 
 geom_col(position = "stack") +
 scale_fill_manual(values = pals::glasbey(15)) +
 labs(title = "Relative frequency of topics over all documents by year")
```

```{r}
XXAW_top |> mutate(year = year(created_time)) |> 
 group_by(year,subreddit) |> summarise(across(paste0("T", 1:15), sum)) |> 
 pivot_longer(cols = paste0("T", 1:15), names_to = "topic", values_to = "count") |>
 mutate(topic = factor(topic, levels = paste0("T", 1:15))) |> 
 ggplot(aes(x = year, y = count, fill=topic)) + 
 geom_col(position = "stack") +
 facet_wrap(~subreddit, ncol = 1, scales = "free_y") +
 scale_fill_manual(values = pals::glasbey(15)) +
 labs(title = "Presence of topics over all documents by year and subreddit")
```

```{r}
XXAW_top |> mutate(year = year(created_time)) |> 
 group_by(year,subreddit) |> summarise(across(paste0("T", 1:15), mean)) |> 
 pivot_longer(cols = paste0("T", 1:15), names_to = "topic", values_to = "count") |>
 mutate(topic = factor(topic, levels = paste0("T", 1:15))) |> 
 ggplot(aes(x = year, y = count, fill=topic)) + 
 geom_col(position = "stack") +
 facet_wrap(~subreddit, ncol = 1, scales = "free_y") +
 scale_fill_manual(values = pals::glasbey(15)) +
 labs(title = "Relative frequency of topics over all documents by year and subreddit")
```


## Users

All users with more than 10 posts in the dataset are shown below. 

```{r}
XXAW |> count(subreddit, author, sort = TRUE)  |> filter(n > 10) |> arrange(subreddit) |> kable()
```

These are users who posted in both subreddits. Shown are the twenty users with most posts in both subreddits.

```{r}
XXusers <- XXAW |> filter(subreddit == "XX") |> count(author, name = "XX_n") 
AWusers <- XXAW |> filter(subreddit == "AW")  |> count(author, name = "AW_n")
XXusers |> inner_join(AWusers, by = "author") |> arrange(desc(XX_n + AW_n)) |> write_csv("XXAWusers.csv")
XXusers |> inner_join(AWusers, by = "author") |> arrange(desc(XX_n + AW_n)) |> head(20) |> kable()
```



```{r}
AceZeroXYZ <- XXAW_top |> filter(author == "AceZeroXYZ") |> 
 select(subreddit, created_time, created_day, url, title, url, num_comments, score, 
        T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14, T15, selftext) 
write_csv(AceZeroXYZ, "AceZeroXYZ.csv")
```





## Appendix

### Perplexity and log-likelihood per word for all LDA models

```{r}
#| echo=FALSE
bind_rows(
 AWl_LDAs |> select(id, perplexity, ll_per_word) |> mutate(id = str_replace(id, "AWl", "Antiwork ")),
 XXl_LDAs |> select(id, perplexity, ll_per_word) |> mutate(id = str_replace(id, "XXl", "TwoXChromosomes ")),
 XXAW15_LDAs |> select(id, perplexity, ll_per_word) |> mutate(id = str_replace(id, "XXAW15l", "Antiwork+TwoXChromosomes ")) 
) |> rename(LDA = id, Perplexity = perplexity, `Log-likelihood per word` = ll_per_word) |>
 knitr::kable()
```